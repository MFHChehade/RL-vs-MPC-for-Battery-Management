{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running A2C_1\n",
      "Running PPO_1.759916063888785\n",
      "Running A2C_2.762324863968786\n",
      "Running PPO_2.374028863728785\n",
      "Running A2C_4.395872863848787\n",
      "Running PPO_4.376131463568786\n",
      "Running A2C_8.403081264548785\n",
      "Running PPO_8.378470364018785\n",
      "Running A2C_16432223563428785\n",
      "Running PPO_16374534963563786\n",
      "1165/2000:-12.384437664548784\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "from environments.energy_management_env import EnergyManagementEnv\n",
    "from environments.env_registration import register_env\n",
    "from rl_monitoring_utils.vectorized_env_wrapper import VectorizedEnvWrapper\n",
    "from policies.categorical_policy import CategoricalPolicy\n",
    "from learning_utils.value_estimator import ValueEstimator\n",
    "from agents.a2c import A2C\n",
    "from agents.ppo import PPO\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define environment parameters and register the environment\n",
    "env_params = {\n",
    "    'SOC_min': 0.2,\n",
    "    'SOC_max': 0.8,\n",
    "    'E': 1000,\n",
    "    'lambda_val': 0.1,\n",
    "    'data_path': 'data/Data_input.csv',\n",
    "    'initial_SOC': 0.5\n",
    "}\n",
    "register_env('EnergyManagement-v0', 'environments.env_registration:environment_creator', {'environment_class': EnergyManagementEnv, **env_params})\n",
    "\n",
    "# Function to run experiments\n",
    "def run_experiment(env, policy_class, agent_class, hidden_sizes, epochs, gamma, T, num_runs):\n",
    "    totals = []\n",
    "    for _ in range(num_runs):\n",
    "        policy = policy_class(env, lr=1e-2, hidden_sizes=hidden_sizes)\n",
    "        value_estimator = ValueEstimator(env, lr=1e-2)\n",
    "        agent, total_rewards = agent_class(env, policy, value_estimator, epochs=epochs, gamma=gamma, T=T)\n",
    "        totals.append(total_rewards)\n",
    "    return totals\n",
    "\n",
    "# Set experiment configurations\n",
    "num_envs_list = [1, 2, 4, 8, 16]\n",
    "hidden_sizes = [16]  # fixed hidden size\n",
    "num_runs = 5\n",
    "epochs = 2000\n",
    "gamma = 1\n",
    "T = 720\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Run experiments for both A2C and PPO with varying num_envs\n",
    "for num_envs in num_envs_list:\n",
    "    energy_management = VectorizedEnvWrapper(gym.make(\"EnergyManagement-v0\"), num_envs=num_envs)\n",
    "    for agent_class, agent_name in [(A2C, 'A2C'), (PPO, 'PPO')]:\n",
    "        label = f'{agent_name}_{num_envs}'\n",
    "        print(f\"Running {label}\")\n",
    "        results[label] = run_experiment(energy_management, CategoricalPolicy, agent_class, hidden_sizes, epochs, gamma, T, num_runs)\n",
    "\n",
    "# Plot results\n",
    "fig, ax = plt.subplots()\n",
    "for label, data in results.items():\n",
    "    means = np.mean(data, axis=0)\n",
    "    stddev = np.std(data, axis=0)\n",
    "    epochs_range = range(len(means))\n",
    "    ax.plot(epochs_range, means, label=label)\n",
    "    ax.fill_between(epochs_range, means - stddev, means + stddev, alpha=0.1)\n",
    "ax.set_title('Performance with Different Number of Environments')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Total Reward')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
