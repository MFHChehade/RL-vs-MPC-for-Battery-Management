{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running A2C_1\n",
      "Running PPO_1.371561963988785\n",
      "Running A2C_12581056864208785\n",
      "Running PPO_12574861964308785\n",
      "Running A2C_12500677064708784\n",
      "Running PPO_12505109564028784\n",
      "1979/2000:-12.397009264328785\r"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import gym\n",
    "from environments.energy_management_env import EnergyManagementEnv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle # save every new element in pickle\n",
    "\n",
    "# Import the Agent class from your code\n",
    "num_envs = 8\n",
    "num_runs = 5\n",
    "epochs = 2000\n",
    "gamma = 1\n",
    "T = 720\n",
    "\n",
    "# Import environment registration function\n",
    "from environments.env_registration import register_env\n",
    "\n",
    "# Define environment parameters\n",
    "env_params = {\n",
    "    'SOC_min': 0.2,\n",
    "    'SOC_max': 0.8,\n",
    "    'E': 1000,\n",
    "    'lambda_val': 0.1,\n",
    "    'data_path': 'data/Data_input.csv',\n",
    "    'initial_SOC': 0.5  # Set to None if not using an initial_SOC\n",
    "}\n",
    "\n",
    "# Register the custom environment\n",
    "register_env('EnergyManagement-v0', 'environments.env_registration:environment_creator',{'environment_class': EnergyManagementEnv, **env_params})\n",
    "\n",
    "from rl_monitoring_utils.vectorized_env_wrapper import VectorizedEnvWrapper\n",
    "from policies.categorical_policy import CategoricalPolicy\n",
    "from learning_utils.value_estimator import ValueEstimator\n",
    "from agents.a2c import A2C\n",
    "from agents.a2c_warm_start import A2C_WarmStart\n",
    "from agents.ppo import PPO\n",
    "from agents.reinforce import REINFORCE\n",
    "import gym\n",
    "\n",
    "\n",
    "energy_management = VectorizedEnvWrapper(gym.make(\"EnergyManagement-v0\"), num_envs=8)\n",
    "\n",
    "def run_experiment(env, policy_class, agent_class, reward_scale, epochs, gamma, T, num_runs):\n",
    "    totals = []\n",
    "    for _ in range(num_runs):\n",
    "        env.reward_scale = reward_scale  # Set the reward scale for the environment\n",
    "        policy = policy_class(env, lr=1e-2)\n",
    "        value_estimator = ValueEstimator(env, lr=1e-2)\n",
    "        agent, total_rewards = agent_class(env, policy, value_estimator, epochs=epochs, gamma=gamma, T=T)\n",
    "        totals.append(total_rewards)\n",
    "    return totals\n",
    "\n",
    "reward_scales = [1, 125, 1250, 12500, 125000]\n",
    "\n",
    "results = {}\n",
    "energy_management = VectorizedEnvWrapper(gym.make(\"EnergyManagement-v0\"), num_envs=num_envs)\n",
    "\n",
    "# Run experiments for both A2C and PPO with varying reward scales\n",
    "for reward_scale in reward_scales:\n",
    "    for agent_class, agent_name in [(A2C, 'A2C'), (PPO, 'PPO')]:\n",
    "        label = f'{agent_name}_{reward_scale}'\n",
    "        print(f\"Running {label}\")\n",
    "        results[label] = run_experiment(energy_management, CategoricalPolicy, agent_class, reward_scale, epochs, gamma, T, num_runs)\n",
    "        # create pickle file first then dump\n",
    "        with open(f\"results/{label}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(results[label], f)\n",
    "\n",
    "# Plot results\n",
    "fig, ax = plt.subplots()\n",
    "for label, data in results.items():\n",
    "    means = np.mean(data, axis=0)\n",
    "    stddev = np.std(data, axis=0)\n",
    "    epochs_range = range(len(means))\n",
    "    ax.plot(epochs_range, means, label=label)\n",
    "    ax.fill_between(epochs_range, means - stddev, means + stddev, alpha=0.1)\n",
    "ax.set_title('Performance with Different Reward Scales')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Total Reward')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from pickle and plot the same thing\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = {}\n",
    "reward_scales = [1, 125, 1250, 12500, 125000]\n",
    "for reward_scale in reward_scales:\n",
    "    for agent_class, agent_name in [(A2C, 'A2C'), (PPO, 'PPO')]:\n",
    "        label = f'{agent_name}_{reward_scale}'\n",
    "        with open(f\"results/{label}.pkl\", 'rb') as f:\n",
    "            results[label] = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
